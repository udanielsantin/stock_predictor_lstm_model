{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb2fd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "import os\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f3e9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all IBOV tickers\n",
    "tickers_df = pd.read_csv(\"../data/ibov_tickers.csv\")\n",
    "tickers = [f\"{ticker}.SA\" for ticker in tickers_df['codigo'].tolist()]\n",
    "\n",
    "print(f\"Total tickers to download: {len(tickers)}\")\n",
    "print(f\"Tickers: {tickers[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcaf793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data for all tickers at once\n",
    "print(\"Downloading data for all tickers...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "data = yf.download(tickers, start=\"2020-01-01\", end=\"2024-12-31\", group_by='ticker', progress=True)\n",
    "\n",
    "print(f\"\\n✓ Data downloaded successfully\")\n",
    "print(f\"Shape: {data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2556c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all tickers' Close prices into single DataFrame\n",
    "all_prices = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        if len(tickers) == 1:\n",
    "            close_data = data['Close']\n",
    "        else:\n",
    "            close_data = data[ticker]['Close']\n",
    "        \n",
    "        if close_data.notna().sum() > 100:  # At least 100 valid data points\n",
    "            df_ticker = close_data.dropna().to_frame()\n",
    "            df_ticker.columns = ['Close']\n",
    "            df_ticker['Ticker'] = ticker\n",
    "            all_prices.append(df_ticker)\n",
    "            print(f\"✓ {ticker}: {len(df_ticker)} days\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ {ticker}: {e}\")\n",
    "\n",
    "# Combine all into one DataFrame\n",
    "combined_df = pd.concat(all_prices, axis=0)\n",
    "combined_df = combined_df.reset_index()\n",
    "combined_df.columns = ['Date', 'Close', 'Ticker']\n",
    "\n",
    "print(f\"\\n✓ Combined dataset shape: {combined_df.shape}\")\n",
    "print(f\"Total data points: {len(combined_df):,}\")\n",
    "print(f\"Successful tickers: {combined_df['Ticker'].nunique()}\")\n",
    "print(f\"Date range: {combined_df['Date'].min()} to {combined_df['Date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff04c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Extract close prices\n",
    "close_prices = combined_df[['Close']].values\n",
    "\n",
    "# Scale data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(close_prices)\n",
    "\n",
    "print(f\"\\n✓ Data scaled to range [0, 1]\")\n",
    "print(f\"Original price range: R$ {close_prices.min():.2f} - R$ {close_prices.max():.2f}\")\n",
    "print(f\"Scaled range: {scaled_data.min():.4f} - {scaled_data.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6144a32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences for LSTM\n",
    "def create_sequences(data, seq_length=50):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i + seq_length])\n",
    "        y.append(data[i + seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_length = 50\n",
    "X, y = create_sequences(scaled_data, seq_length)\n",
    "\n",
    "print(f\"✓ Created sequences with window size: {seq_length}\")\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"y shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b23f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split (80/20)\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.FloatTensor(X_train).to(device)\n",
    "y_train = torch.FloatTensor(y_train).to(device)\n",
    "X_test = torch.FloatTensor(X_test).to(device)\n",
    "y_test = torch.FloatTensor(y_test).to(device)\n",
    "\n",
    "print(f\"✓ Data split completed\")\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"Train/Test ratio: {X_train.shape[0]/X_test.shape[0]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14607ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LSTM Model\n",
    "class UnifiedLSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=128, num_layers=3, dropout=0.2):\n",
    "        super(UnifiedLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, \n",
    "            hidden_size, \n",
    "            num_layers, \n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Create model\n",
    "model = UnifiedLSTM(input_size=1, hidden_size=128, num_layers=3, dropout=0.2).to(device)\n",
    "\n",
    "print(\"✓ Unified LSTM Model Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0711e38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING UNIFIED MODEL\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Epochs: {epochs}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Optimizer: Adam (lr=0.001)\")\n",
    "print(f\"Loss: MSE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01e06c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Create DataLoader for batch training\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "model.train()\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0.0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        batch_count += 1\n",
    "    \n",
    "    # Calculate average training loss\n",
    "    avg_train_loss = epoch_loss / batch_count\n",
    "    history['train_loss'].append(avg_train_loss)\n",
    "    \n",
    "    # Validation loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_test)\n",
    "        val_loss = criterion(val_outputs, y_test).item()\n",
    "        history['val_loss'].append(val_loss)\n",
    "    model.train()\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"Epoch [{epoch+1:3d}/{epochs}] - \"\n",
    "              f\"Train Loss: {avg_train_loss:.6f}, \"\n",
    "              f\"Val Loss: {val_loss:.6f}, \"\n",
    "              f\"Time: {elapsed:.1f}s\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\n✓ Training completed in {total_time:.1f} seconds ({total_time/60:.1f} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0a041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "ax.plot(history['train_loss'], label='Training Loss', linewidth=2)\n",
    "ax.plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss (MSE)', fontsize=12)\n",
    "ax.set_title('Unified IBOV Model - Training History', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Training Loss: {history['train_loss'][-1]:.6f}\")\n",
    "print(f\"Final Validation Loss: {history['val_loss'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b7cc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test).cpu().numpy()\n",
    "    y_true = y_test.cpu().numpy()\n",
    "\n",
    "# Calculate metrics on scaled data\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = math.sqrt(mse)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "mape = np.mean(np.abs((y_true - y_pred) / np.clip(y_true, 1e-8, None))) * 100\n",
    "\n",
    "# Inverse transform to get real prices\n",
    "y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "y_true_inv = scaler.inverse_transform(y_true)\n",
    "\n",
    "# Calculate metrics on real prices\n",
    "rmse_real = math.sqrt(mean_squared_error(y_true_inv, y_pred_inv))\n",
    "mae_real = mean_absolute_error(y_true_inv, y_pred_inv)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nScaled Data Metrics:\")\n",
    "print(f\"  MSE:  {mse:.6f}\")\n",
    "print(f\"  RMSE: {rmse:.6f}\")\n",
    "print(f\"  MAE:  {mae:.6f}\")\n",
    "print(f\"  R²:   {r2:.6f}\")\n",
    "print(f\"  MAPE: {mape:.2f}%\")\n",
    "\n",
    "print(\"\\nReal Price Metrics:\")\n",
    "print(f\"  RMSE: R$ {rmse_real:.4f}\")\n",
    "print(f\"  MAE:  R$ {mae_real:.4f}\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7995decc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions\n",
    "sample_size = 500  # Show last 500 predictions\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Full test set predictions\n",
    "axes[0].plot(y_true_inv, label='Actual', color='blue', linewidth=1.5, alpha=0.7)\n",
    "axes[0].plot(y_pred_inv, label='Predicted', color='orange', linewidth=1.5, alpha=0.7)\n",
    "axes[0].set_title('Unified Model - Full Test Set Predictions', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Sample Index')\n",
    "axes[0].set_ylabel('Price (R$)')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add metrics text\n",
    "metrics_text = (f\"R² = {r2:.4f}\\n\"\n",
    "                f\"RMSE = R$ {rmse_real:.4f}\\n\"\n",
    "                f\"MAE = R$ {mae_real:.4f}\\n\"\n",
    "                f\"MAPE = {mape:.2f}%\")\n",
    "axes[0].text(0.02, 0.98, metrics_text, transform=axes[0].transAxes,\n",
    "            verticalalignment='top', fontsize=10,\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "# Plot 2: Zoomed in on last N samples\n",
    "axes[1].plot(y_true_inv[-sample_size:], label='Actual', color='blue', linewidth=2, marker='o', markersize=3)\n",
    "axes[1].plot(y_pred_inv[-sample_size:], label='Predicted', color='orange', linewidth=2, marker='s', markersize=3, alpha=0.7)\n",
    "axes[1].set_title(f'Zoomed View - Last {sample_size} Predictions', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Sample Index')\n",
    "axes[1].set_ylabel('Price (R$)')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d08e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction error analysis\n",
    "errors = y_true_inv - y_pred_inv\n",
    "errors_pct = (errors / y_true_inv) * 100\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "# Error distribution\n",
    "axes[0].hist(errors.flatten(), bins=50, color='skyblue', edgecolor='black')\n",
    "axes[0].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "axes[0].set_title('Prediction Error Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Error (R$)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Percentage error distribution\n",
    "axes[1].hist(errors_pct.flatten(), bins=50, color='lightcoral', edgecolor='black')\n",
    "axes[1].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "axes[1].set_title('Percentage Error Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Error (%)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot: Predicted vs Actual\n",
    "axes[2].scatter(y_true_inv, y_pred_inv, alpha=0.3, s=10)\n",
    "min_val = min(y_true_inv.min(), y_pred_inv.min())\n",
    "max_val = max(y_true_inv.max(), y_pred_inv.max())\n",
    "axes[2].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[2].set_title('Predicted vs Actual Prices', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Actual Price (R$)')\n",
    "axes[2].set_ylabel('Predicted Price (R$)')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean Error: R$ {errors.mean():.4f}\")\n",
    "print(f\"Std Error: R$ {errors.std():.4f}\")\n",
    "print(f\"Mean Absolute % Error: {np.abs(errors_pct).mean():.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20937390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the unified model and scaler\n",
    "os.makedirs(\"../models\", exist_ok=True)\n",
    "os.makedirs(\"../scalers\", exist_ok=True)\n",
    "\n",
    "model_path = \"../models/unified_ibov_lstm.pt\"\n",
    "scaler_path = \"../scalers/unified_ibov_scaler.joblib\"\n",
    "\n",
    "torch.save(model.state_dict(), model_path)\n",
    "joblib.dump(scaler, scaler_path)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL SAVED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Model: {model_path}\")\n",
    "print(f\"Scaler: {scaler_path}\")\n",
    "print(f\"\\nModel can be loaded with:\")\n",
    "print(\"  model = UnifiedLSTM(...)\")\n",
    "print(\"  model.load_state_dict(torch.load('models/unified_ibov_lstm.pt'))\")\n",
    "print(\"  scaler = joblib.load('scalers/unified_ibov_scaler.joblib')\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc379d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training summary\n",
    "summary = {\n",
    "    'model_type': 'Unified LSTM for All IBOV Tickers',\n",
    "    'training_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'data_range': f\"{combined_df['Date'].min()} to {combined_df['Date'].max()}\",\n",
    "    'total_tickers': combined_df['Ticker'].nunique(),\n",
    "    'total_samples': len(combined_df),\n",
    "    'train_samples': len(X_train),\n",
    "    'test_samples': len(X_test),\n",
    "    'sequence_length': seq_length,\n",
    "    'epochs': epochs,\n",
    "    'batch_size': batch_size,\n",
    "    'architecture': {\n",
    "        'input_size': 1,\n",
    "        'hidden_size': 128,\n",
    "        'num_layers': 3,\n",
    "        'dropout': 0.2\n",
    "    },\n",
    "    'metrics': {\n",
    "        'R2': float(r2),\n",
    "        'RMSE': float(rmse_real),\n",
    "        'MAE': float(mae_real),\n",
    "        'MAPE': float(mape)\n",
    "    },\n",
    "    'training_time_minutes': total_time / 60\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame([summary])\n",
    "summary_df.to_csv('../data/unified_model_summary.csv', index=False)\n",
    "\n",
    "print(\"✓ Training summary saved to: data/unified_model_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536b75fd",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This unified model was trained on **ALL IBOV tickers combined** into a single dataset. \n",
    "\n",
    "**Advantages:**\n",
    "- Single model learns general market patterns\n",
    "- Faster inference (no need to load multiple models)\n",
    "- Learns cross-stock relationships\n",
    "\n",
    "**Model can be used to:**\n",
    "- Predict stock prices for any IBOV ticker\n",
    "- Analyze market trends\n",
    "- Make future predictions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
