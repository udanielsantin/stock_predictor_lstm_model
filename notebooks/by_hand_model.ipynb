{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e842d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import lightning as L\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8eab82a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMbyHand(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        mean = torch.tensor(0.0)\n",
    "        std = torch.tensor(1.0)\n",
    "\n",
    "        self.wlr1 = nn.Parameter(torch.normal(mean = mean, std = std), requires_grad=True)\n",
    "        self.wlr2 = nn.Parameter(torch.normal(mean = mean, std = std), requires_grad=True)\n",
    "        self.blr1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "        \n",
    "        self.wpr1 = nn.Parameter(torch.normal(mean = mean, std = std), requires_grad=True)\n",
    "        self.wpr2 = nn.Parameter(torch.normal(mean = mean, std = std), requires_grad=True)\n",
    "        self.bpr1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        self.wp1 = nn.Parameter(torch.normal(mean = mean, std = std), requires_grad=True)\n",
    "        self.wp2 = nn.Parameter(torch.normal(mean = mean, std = std), requires_grad=True)\n",
    "        self.bp1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "        self.wo1 = nn.Parameter(torch.normal(mean = mean, std = std), requires_grad=True)\n",
    "        self.wo2 = nn.Parameter(torch.normal(mean = mean, std = std), requires_grad=True)\n",
    "        self.bo1 = nn.Parameter(torch.tensor(0.), requires_grad=True)\n",
    "\n",
    "\n",
    "    def lstm_unit(self, input_value, long_memory, short_memory):\n",
    "        long_remember_percent = torch.sigmoid((short_memory * self.wlr1) \n",
    "                                              + (input_value * self.wlr2)\n",
    "                                              + self.blr1)\n",
    "        \n",
    "        potential_remember_percent = torch.sigmoid((short_memory * self.wpr1)\n",
    "                                                    + (input_value * self.wpr2)\n",
    "                                                    + self.bpr1)\n",
    "        \n",
    "        potential_memory = torch.tanh((short_memory * self.wp1)\n",
    "                                      + (input_value * self.wp2)\n",
    "                                      + self.bp1)\n",
    "\n",
    "        updated_long_memory = ((long_memory * long_remember_percent) + (potential_memory * potential_remember_percent))\n",
    "\n",
    "        output_percent = torch.sigmoid((short_memory * self.wo1)\n",
    "                                       + (input_value * self.wo2)\n",
    "                                       + self.bo1)\n",
    "        \n",
    "        updated_short_memory = torch.tanh(updated_long_memory) * output_percent\n",
    "\n",
    "        return ([updated_long_memory, updated_short_memory])\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        long_memory = 0\n",
    "        short_memory = 0\n",
    "        day1 = input[0]\n",
    "        day2 = input[1]\n",
    "        day3 = input[2]\n",
    "        day4 = input[3]\n",
    "\n",
    "        long_memory, short_memory = self.lstm_unit(day1, long_memory, short_memory)\n",
    "        long_memory, short_memory = self.lstm_unit(day2, long_memory, short_memory)\n",
    "        long_memory, short_memory = self.lstm_unit(day3, long_memory, short_memory)\n",
    "        long_memory, short_memory = self.lstm_unit(day4, long_memory, short_memory)\n",
    "\n",
    "        return short_memory\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "    \n",
    "        return Adam(self.parameters())\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        input_i, label_i = batch\n",
    "        output_i = self.forward(input_i[0])\n",
    "        loss = (output_i - label_i)**2\n",
    "\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        if (label_i == 0):\n",
    "            self.log(\"out_0\",output_i)\n",
    "        else:\n",
    "            self.log(\"out_1\",output_i)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caf271be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Company A: Observer = 0 , Predicted =  tensor(-0.3238)\n",
      "Company B: Observer = 1 , Predicted =  tensor(-0.3531)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = LSTMbyHand()\n",
    "\n",
    "print(\"Company A: Observer = 0 , Predicted = \", model(torch.tensor([0.,0.5,0.25,1.])).detach())\n",
    "\n",
    "print(\"Company B: Observer = 1 , Predicted = \", model(torch.tensor([1,0.5,0.25,1.])).detach())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa453d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "2025-11-23 13:30:00.428927: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "  | Name         | Type | Params | Mode\n",
      "---------------------------------------------\n",
      "  | other params | n/a  | 12     | n/a \n",
      "---------------------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/workspaces/stock_predictor_lstm_model/venv/lib/python3.12/site-packages/lightning/pytorch/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1999: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 130.67it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1999: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 96.42it/s, v_num=0] \n",
      "Company A: Observer = 0 , Predicted =  tensor(0.5032)\n",
      "Company B: Observer = 1 , Predicted =  tensor(0.5148)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = torch.tensor([[0.,0.5,0.25,1.],[1,0.5,0.25,1.]])\n",
    "\n",
    "labels = torch.tensor([0.,1.])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels)\n",
    "dataloader = DataLoader(dataset)\n",
    "\n",
    "trainer = L.Trainer(max_epochs=2000)\n",
    "trainer.fit(model, train_dataloaders=dataloader)\n",
    "\n",
    "print(\"Company A: Observer = 0 , Predicted = \", model(torch.tensor([0.,0.5,0.25,1.])).detach())\n",
    "\n",
    "print(\"Company B: Observer = 1 , Predicted = \", model(torch.tensor([1,0.5,0.25,1.])).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1bfaeaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Restoring states from the checkpoint path at /workspaces/stock_predictor_lstm_model/notebooks/lightning_logs/version_0/checkpoints/epoch=1999-step=4000.ckpt\n",
      "/workspaces/stock_predictor_lstm_model/venv/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:445: The dirpath has changed from '/workspaces/stock_predictor_lstm_model/notebooks/lightning_logs/version_0/checkpoints' to '/workspaces/stock_predictor_lstm_model/notebooks/lightning_logs/version_1/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "\n",
      "  | Name         | Type | Params | Mode\n",
      "---------------------------------------------\n",
      "  | other params | n/a  | 12     | n/a \n",
      "---------------------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Restored all states from the checkpoint at /workspaces/stock_predictor_lstm_model/notebooks/lightning_logs/version_0/checkpoints/epoch=1999-step=4000.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3999: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 136.29it/s, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3999: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 99.67it/s, v_num=1] \n",
      "Company A: Observer = 0 , Predicted =  tensor(0.4892)\n",
      "Company B: Observer = 1 , Predicted =  tensor(0.5313)\n"
     ]
    }
   ],
   "source": [
    "path_to_best_checkpoint = trainer.checkpoint_callback.best_model_path\n",
    "\n",
    "trainer = L.Trainer(max_epochs=4000)\n",
    "trainer.fit(model, train_dataloaders=dataloader, ckpt_path=path_to_best_checkpoint)\n",
    "\n",
    "print(\"Company A: Observer = 0 , Predicted = \", model(torch.tensor([0.,0.5,0.25,1.])).detach())\n",
    "\n",
    "print(\"Company B: Observer = 1 , Predicted = \", model(torch.tensor([1,0.5,0.25,1.])).detach())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9185ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Restoring states from the checkpoint path at /workspaces/stock_predictor_lstm_model/notebooks/lightning_logs/version_1/checkpoints/epoch=3999-step=8000.ckpt\n",
      "/workspaces/stock_predictor_lstm_model/venv/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:445: The dirpath has changed from '/workspaces/stock_predictor_lstm_model/notebooks/lightning_logs/version_1/checkpoints' to '/workspaces/stock_predictor_lstm_model/notebooks/lightning_logs/version_2/checkpoints', therefore `best_model_score`, `kth_best_model_path`, `kth_value`, `last_model_path` and `best_k_models` won't be reloaded. Only `best_model_path` will be reloaded.\n",
      "\n",
      "  | Name         | Type | Params | Mode\n",
      "---------------------------------------------\n",
      "  | other params | n/a  | 12     | n/a \n",
      "---------------------------------------------\n",
      "12        Trainable params\n",
      "0         Non-trainable params\n",
      "12        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Restored all states from the checkpoint at /workspaces/stock_predictor_lstm_model/notebooks/lightning_logs/version_1/checkpoints/epoch=3999-step=8000.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4999: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 114.86it/s, v_num=2]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4999: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 85.33it/s, v_num=2] \n",
      "Company A: Observer = 0 , Predicted =  tensor(0.4440)\n",
      "Company B: Observer = 1 , Predicted =  tensor(0.5764)\n"
     ]
    }
   ],
   "source": [
    "path_to_best_checkpoint = trainer.checkpoint_callback.best_model_path\n",
    "\n",
    "trainer = L.Trainer(max_epochs=5000)\n",
    "trainer.fit(model, train_dataloaders=dataloader, ckpt_path=path_to_best_checkpoint)\n",
    "\n",
    "print(\"Company A: Observer = 0 , Predicted = \", model(torch.tensor([0.,0.5,0.25,1.])).detach())\n",
    "\n",
    "print(\"Company B: Observer = 1 , Predicted = \", model(torch.tensor([1,0.5,0.25,1.])).detach())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
